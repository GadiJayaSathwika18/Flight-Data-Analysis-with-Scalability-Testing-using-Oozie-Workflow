In this document, you will find a comprehensive set of commands necessary for the successful execution of our Big Data project. These commands are designed to manage and optimize various aspects of the project, ensuring seamless functionality and efficient processing of large datasets.

Please refer to the following sections for detailed instructions and guidelines on running the commands effectively.


Prerequisite - 

1. Creating a AWS instance (master and slaves) with the below specifications -

   Instance Information: Ubuntu Server 20.04 LTS (HVM)
   Type: t2.medium
   Memory(GiB): 4
   Instance Storage(GB): 24G

2. Install Java on the AWS EC2 instance - (https://www.eternalsoftsolutions.com/blog/how-to-install-java-in-aws-ec2/)
3. Install HAPDOOP on the AWS EC2 instance - (https://medium.com/@sedaatalay/setting-up-hadoop-to-aws-ec2-and-running-a-sample-mapreduce-5ec6ac623d31)
4. Install OOZIE on the AWS EC2 instance -  (https://abizeradenwala.blogspot.com/2015/07/installing-oozie-and-running-sample-job.html)



Commands to run in Fully distributed Mode - 

Step1 : Format the Namenode initially and commence the Hadoop service - 

Initiate the process by formatting the Namenode and launching the Hadoop service to establish a robust foundation for efficient data processing and storage within the system.

   $bin/hdfs namenode -format
   $sbin/start-dfs.sh
   $sbin/start-yarn.sh


Step2 : Compile the programs and create JAR package using the instructions provided below -

Here make sure JAR packages are created for all the java programs.

   $javac <file_name.java> -cp $(Hadoop classpath)
   $jar cvf <file_name.jar> *.class


Step3 : Transfer the input data files to HDFS by creating an input directory - 

Then we proceed by transfering the input data files to HDFS by establishing an input directory, ensuring seamless integration and accessibility for further processing and analysis on the Hadoop Distributed File System.

    $bin/hdfs dfs  -mkdir  /input
    $bin/hdfs dfs -put flight-data input/


Step4 : OOZIE start -

Then proceed to start OOZIE using the below command

   $OOZIE_HOME/bin/oozied.sh start



Step5 : Run the Programs -

   oozie job -oozie http://ec2-44-204-154-15.compute-1.amazonaws.com:11000/oozie/ -config flight-data/apps/map-reduce/job.properties -run


Step6 : Use the WEB URL to check Oozie status -

Then we navigating to the specified web URL, where we can assess the current status and operational health of the Oozie service.

   http://ec2-44-204-154-15.compute-1.amazonaws.com:11000/oozie/


Step7 : Get Result:

Gain access to the desired output or information, ensuring the successful running of the intended task.
$ hdfs dfs -get flight-data/output output

14. See the result:
$ cat output/OnScheduleAirlines/part-r-00000
$ cat output/AirportsTaxiTime/part-r-00000
$ cat output/Cancellations/part-r-00000

